# Models Directory

This directory contains trained machine learning models and related files.

## Generated Files:

### sentiment_model.pkl
- **Description**: Trained sentiment analysis model
- **Generated by**: `train_model.py` script
- **Format**: Pickled scikit-learn model
- **Model Types**: Logistic Regression, Random Forest, or SVM
- **Usage**: Loaded by the Streamlit app for ML-based sentiment analysis

### vectorizer.pkl
- **Description**: Fitted TF-IDF vectorizer
- **Generated by**: `train_model.py` script
- **Format**: Pickled scikit-learn TfidfVectorizer
- **Configuration**:
  - Max features: 10,000
  - N-gram range: (1, 2)
  - Min document frequency: 2
  - Max document frequency: 0.95
- **Usage**: Text preprocessing for model predictions

## Model Training Process:

1. **Data Preprocessing**: Clean and tokenize text data
2. **Feature Extraction**: Create TF-IDF features
3. **Model Training**: Train multiple models and select the best
4. **Model Evaluation**: Assess performance on test data
5. **Model Saving**: Save the best model and vectorizer

## Model Performance:

The training script evaluates models using:
- Accuracy score
- Precision and recall for both classes
- F1-score
- Confusion matrix

## Usage:

### Training Models:
```bash
python train_model.py
```

### Loading Models in Code:
```python
from utils.sentiment_analyzer import SentimentAnalyzer

analyzer = SentimentAnalyzer()
analyzer.load_model('models/sentiment_model.pkl', 'models/vectorizer.pkl')
```

## File Structure:
```
models/
├── sentiment_model.pkl       # Trained ML model (auto-generated)
├── vectorizer.pkl           # Fitted TF-IDF vectorizer (auto-generated)
├── __init__.py              # Python package init file
└── README.md                # This file
```

## Model Requirements:

- **Training Data**: IMDB Dataset.csv in data/ directory
- **Dependencies**: All packages from requirements.txt
- **Memory**: ~2GB RAM for training
- **Time**: 5-15 minutes depending on hardware